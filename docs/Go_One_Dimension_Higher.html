<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Go One Dimension Higher: Can Neural Networks for Point Cloud Analysis Help in Image Recognition?
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @charset "UTF-8";@import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{display:block;background:#fff;padding:.5em;color:#333;overflow-x:auto}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{color:#55a532;background-color:#eaffea}.hljs-deletion{color:#bd2c00;background-color:#ffecec}.hljs-link{text-decoration:underline}.markdown-body{font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{font-size:16px;padding:0 1em;color:#777;border-left:.25em solid #ddd}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd,.popover kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid #ccc;border-bottom-color:#bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{padding-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\00a0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0 none}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important;position:relative}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{text-align:center;background-color:inherit;border-radius:0;white-space:inherit;overflow:visible}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{max-width:100%;height:100%}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.MJX_Assistive_MathML{display:none}.ui-infobar{position:relative;z-index:2;max-width:760px;margin:25px auto -25px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:998}.ui-toc.both-mode{margin-left:8px}.ui-toc.both-mode .ui-toc-label{height:40px;padding:10px 4px;border-top-left-radius:0;border-bottom-left-radius:0}.ui-toc-label{background-color:#e6e6e6;border:none;color:#868686;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#fff;transition:opacity .2s}.ui-toc-label:focus{opacity:.3;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;transition:opacity .2s}.ui-toc-dropdown{margin-top:20px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child > ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif}html[lang^=ja] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html .markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html .markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html .markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html[lang^=ja] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html[lang=zh-tw] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html[lang=zh-cn] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}html .ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html .ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html .ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:#999}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:3px;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:cover}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.ui-published-note{color:#337ab7}.ui-published-note .fa{font-size:20px;vertical-align:top}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}.selectable{-webkit-user-select:text;-o-user-select:text;user-select:text}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;letter-spacing:.025em}.focus,:focus{outline:none!important}::-moz-focus-inner{border:0!important}body{font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] body{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}abbr[data-original-title],abbr[title]{cursor:help}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled comment-open comment-open-inner" data-hard-breaks="true"><h1 id="Go-One-Dimension-Higher-Can-Neural-Networks-for-Point-Cloud-Analysis-Help-in-Image-Recognition" data-id="Go-One-Dimension-Higher-Can-Neural-Networks-for-Point-Cloud-Analysis-Help-in-Image-Recognition" data-original-title="" title=""><a class="anchor hidden-xs" href="#Go-One-Dimension-Higher-Can-Neural-Networks-for-Point-Cloud-Analysis-Help-in-Image-Recognition" title="Go-One-Dimension-Higher-Can-Neural-Networks-for-Point-Cloud-Analysis-Help-in-Image-Recognition"><span class="octicon octicon-link"></span></a><span>Go One Dimension Higher: Can Neural Networks for Point Cloud Analysis Help in Image Recognition?</span></h1><p><em><span>Zhaiyu Chen</span></em><span> and </span><em><span>Qian Bai</span></em></p><p><img src="https://i.imgur.com/zDdbQei.png" alt=""></p><hr><p><span>Inspired by a sanity check made by Qi et al.</span><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup><span> using PointNet, which treats an image from MNIST dataset as a 2D point cloud, in this project we discuss if neural networks designed for point cloud analysis can help image classification.</span></p><blockquote>
<p><em><span>While we focus on 3D point cloud learning, a sanity check experiment is to apply our network on a 2D point clouds - pixel sets </span><sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup><span>.</span></em></p>
</blockquote><p><span>Compared to images, point clouds are unordered and imply geometric information of the object. Deep learning architectures like PointNet can directly consume raw point clouds and select informative points for semantic labeling. Such networks also preserve the permutation invariance property of point clouds, which is not present in convolutional neural networks (CNNs) for images.</span></p><p><span>In this project, we choose three deep learning networks for point cloud processing, which are PointNet, PointNet++ and PointCNN. These architectures show excellent performance on point cloud labeling and can be considered interesting for image classification. Certainly, we are not aiming to achieve state-of-the-art performance using networks designed for point clouds, since the complexity of these chosen networks is not comparable to deep 2D CNNs. Instead, we would focus on if PointNet, PointNet++ and PointCNN can also do well on image tasks based on their advantages of processing geometric data and which image features are the most important for their performances.</span></p><p><span>The main contributions of our project can be summarized as the following:</span></p><ul>
<li><span>We implemened a unified PyTorch framework for all the experiments, which is based on existing Pytorch code on </span><a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch" target="_blank" rel="noopener"><span>PointNet</span></a><span>, </span><a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch" target="_blank" rel="noopener"><span>PointNet++</span></a><span> and </span><a href="https://github.com/rusty1s/pytorch_geometric" target="_blank" rel="noopener"><span>Pytorch Geometric</span></a><span>. Our code can be found in the </span><a href="https://github.com/chenzhaiyu/pointset-image-recognition" target="_blank" rel="noopener"><span>Github Repository</span></a><span>.</span></li>
<li><span>We extended image datasets used in the original sanity check for point cloud classification on PointNet, PointNet++ and PointCNN (MNIST and CIFAR10) by using one additional dataset – Fashion MNIST.</span></li>
<li><span>When exploring the impact of different image features, we applied Grabcut on CIFAR10 dataset to extract the shape information and compared the importance of shape and RGB features.</span></li>
<li><span>We discussed whether PointCNN shows the property of shift invariance.</span></li>
<li><span>We extended the usecase of PointNet and PointNet++ to image semantic segmentation.</span></li>
</ul><h2 id="Related-Work" data-id="Related-Work"><a class="anchor hidden-xs" href="#Related-Work" title="Related-Work"><span class="octicon octicon-link"></span></a><span>Related Work</span></h2><h3 id="Image-Recognition" data-id="Image-Recognition"><a class="anchor hidden-xs" href="#Image-Recognition" title="Image-Recognition"><span class="octicon octicon-link"></span></a><span>Image Recognition</span></h3><p><span>Image recognition has reached the mainstream and is even deployed by some industries. Many visual recognition tasks, if not all, have benefited from the convolutional neural networks (CNNs), which progressively extract higher- and higher-level representations of the image content. Instead of handcrafting features like textures and shapes, a CNN takes the image’s raw pixels as input and extracts the features, and ultimately infers the image semantics.</span></p><p><span>For example, VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman </span><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup><span>. The model achieves 92.7% top-5 test accuracy in ImageNet, a dataset of over 14 million images belonging to 1000 classes. </span><em><span>Figure 1</span></em><span> illustrates the architecture of VGG16.</span></p><p><img src="https://i.imgur.com/I0DXDir.png" alt=""><br>
<em><span>Figure 1: VGG16 Architecture. Taken from </span><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup><span>.</span></em></p><p><span>Besides VGG16, other CNNs are used in this project as baselines including Maxout Network, ResNet, etc.</span></p><h3 id="Point-Cloud-Analysis" data-id="Point-Cloud-Analysis"><a class="anchor hidden-xs" href="#Point-Cloud-Analysis" title="Point-Cloud-Analysis"><span class="octicon octicon-link"></span></a><span>Point Cloud Analysis</span></h3><p><span>Recently, point cloud, as an important source of 3D data, is exploited in the deep learning community. Point cloud data is inherently embedded in 3D space and has rich semantic representations. Next we’ll introduce the three neural networks we exploited in this project, namely PointNet, PointNet++ and PointCNN.</span></p><h4 id="PointNet" data-id="PointNet"><a class="anchor hidden-xs" href="#PointNet" title="PointNet"><span class="octicon octicon-link"></span></a><span>PointNet</span></h4><p><span>PointNet exploits features by extracting a global feature from all point cloud data, which substantially neglect the local embedding of the point features</span><sup class="footnote-ref"><a href="#fn1" id="fnref1:2">[1:2]</a></sup><span>. </span><em><span>Figure 2</span></em><span> shows the architecture of PointNet.</span></p><p><img src="https://i.imgur.com/tFMf1iQ.png" alt=""><br>
<em><span>Figure 2: PointNet Architecture. Taken from</span><sup class="footnote-ref"><a href="#fn1" id="fnref1:3">[1:3]</a></sup><span>.</span></em></p><p><span>The input points are first aligned by multiplying a transformation network called T-Net, which resembles a mini PointNet and encodes the geometric transformation upon the point set by learning the conversion matrix to ensure the invariance of the model. After extracting the features of each point cloud through multiple multi-layer perceptrons (MLPs), a T-Net is used again to align the features. Then max-pooling operations are performed on each dimension of the feature to get the final global feature. For classification tasks, the global features are predicted by an MLP to predict the final classification score; for segmentation tasks, the global features are concatenated with the previously learned local features of each point cloud to encode the per-point features, and then the classification results of each data point are obtained by another MLP.</span></p><h4 id="PointNet1" data-id="PointNet"><a class="anchor hidden-xs" href="#PointNet1" title="PointNet1"><span class="octicon octicon-link"></span></a><span>PointNet++</span></h4><p><span>Inspired by CNNs’ layer-by-layer abstraction of local features, PointNet++ can extract local features at different scales and obtain deep features through a multi-layer network structure</span><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup><span>. </span><em><span>Figure 3</span></em><span> shows the architecture of the PointNet++.</span></p><p><img src="https://i.imgur.com/sZxSmRK.png" alt=""><br>
<em><span>Figure 3: PointNet++ Architecture. Taken from</span><sup class="footnote-ref"><a href="#fn4" id="fnref4:1">[4:1]</a></sup><span>.</span></em></p><p><span>There are three key components in PointNet++, namely sampling and grouping, feature learning. Farthest point sampling (FPS) is performed to sample the data points, which can better cover the entire sampling space compared with random sampling. To extract the local features of a point, the “locality” of a point is partially composed of other points in the spherical space drawn by a given radius around it to facilitate the subsequent feature extraction for each local spatial neighbourhood. PointNet provides a feature extraction network based on point cloud data, which is directly to extract local features.</span></p><p><span>The above components constitute the basic processing module of PointNet++. If multiple such processing modules are cascaded together, PointNet++ can get deep semantic features from shallow features like CNNs. For segmentation tasks, it is also necessary to upsample the features after downsampling, so that each point in the original point cloud is assigned with corresponding features.</span></p><h4 id="PointCNN" data-id="PointCNN"><a class="anchor hidden-xs" href="#PointCNN" title="PointCNN"><span class="octicon octicon-link"></span></a><span>PointCNN</span></h4><p><span>For a typical convolution operation, the output changes with the order of the input points, which is an undesired property. Therefore, the input order of the points in the point cloud is the main problem that hinders the operation of convolution. To address the issue, PointCNN defines a transformation matrix </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-70-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1234" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1235" class="mjx-mrow"><span id="MJXc-Node-1236" class="mjx-texatom"><span id="MJXc-Node-1237" class="mjx-mrow"><span id="MJXc-Node-1238" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-70">\mathcal{X}</script></span><span>, which can process the input points with an arbitrary order to obtain a feature that is order-independent. </span><em><span>Figure 4</span></em><span> details the </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-71-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1239" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1240" class="mjx-mrow"><span id="MJXc-Node-1241" class="mjx-texatom"><span id="MJXc-Node-1242" class="mjx-mrow"><span id="MJXc-Node-1243" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-71">\mathcal{X}</script></span><span>-Conv operator, where </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-72-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>P</mtext></mrow><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>p</mi><mi>k</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mi>T</mi></msup></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1244" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1245" class="mjx-mrow"><span id="MJXc-Node-1246" class="mjx-texatom"><span id="MJXc-Node-1247" class="mjx-mrow"><span id="MJXc-Node-1248" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">P</span></span></span></span><span id="MJXc-Node-1249" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.058em; padding-bottom: 0.335em;">=</span></span><span id="MJXc-Node-1250" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1251" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-1252" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-1253" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">1</span></span></span></span><span id="MJXc-Node-1254" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1255" class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span id="MJXc-Node-1256" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-1257" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">2</span></span></span></span><span id="MJXc-Node-1258" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1259" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.335em;">.</span></span><span id="MJXc-Node-1260" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.335em;">.</span></span><span id="MJXc-Node-1261" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.335em;">.</span></span><span id="MJXc-Node-1262" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1263" class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span id="MJXc-Node-1264" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span id="MJXc-Node-1265" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">k</span></span></span></span><span id="MJXc-Node-1266" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-1267" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-1268" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.12em;">T</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">P</mtext></mrow><mo>=</mo><mo stretchy="false">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>p</mi><mi>k</mi></msub><msup><mo stretchy="false">)</mo><mi>T</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-72">\textbf{P}=(p_1, p_2, ..., p_k)^T</script></span><span> is the neighbouring points with </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-73-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>F</mtext></mrow><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>f</mi><mi>k</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mi>T</mi></msup></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1269" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1270" class="mjx-mrow"><span id="MJXc-Node-1271" class="mjx-texatom"><span id="MJXc-Node-1272" class="mjx-mrow"><span id="MJXc-Node-1273" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">F</span></span></span></span><span id="MJXc-Node-1274" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.058em; padding-bottom: 0.335em;">=</span></span><span id="MJXc-Node-1275" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1276" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span id="MJXc-Node-1277" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.501em; padding-right: 0.06em;">f</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-1278" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">1</span></span></span></span><span id="MJXc-Node-1279" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1280" class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.06em;"><span id="MJXc-Node-1281" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.501em; padding-right: 0.06em;">f</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-1282" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">2</span></span></span></span><span id="MJXc-Node-1283" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1284" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.335em;">.</span></span><span id="MJXc-Node-1285" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.335em;">.</span></span><span id="MJXc-Node-1286" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.335em;">.</span></span><span id="MJXc-Node-1287" class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1288" class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.06em;"><span id="MJXc-Node-1289" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.501em; padding-right: 0.06em;">f</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span id="MJXc-Node-1290" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">k</span></span></span></span><span id="MJXc-Node-1291" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-1292" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-1293" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.12em;">T</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">F</mtext></mrow><mo>=</mo><mo stretchy="false">(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>f</mi><mi>k</mi></msub><msup><mo stretchy="false">)</mo><mi>T</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-73">\textbf{F}=(f_1, f_2, ..., f_k)^T</script></span><span> as their features. </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-74-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>K</mtext></mrow></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1294" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1295" class="mjx-mrow"><span id="MJXc-Node-1296" class="mjx-texatom"><span id="MJXc-Node-1297" class="mjx-mrow"><span id="MJXc-Node-1298" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">K</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">K</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-74">\textbf{K}</script></span><span> represents the convolution kernel and </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-75-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1299" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1300" class="mjx-mrow"><span id="MJXc-Node-1301" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></span></span><script type="math/tex" id="MathJax-Element-75">p</script></span><span> represents the representative points. The operation can also be expressed more concisely with</span></p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="mjx-chtml MJXc-display" style="text-align: center;"><span id="MathJax-Element-76-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>F</mtext></mrow><mi>p</mi></msub><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow><mo>&amp;#x2212;</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>K</mtext></mrow><mo>,</mo><mi>p</mi><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>P</mtext></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>F</mtext></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>K</mtext></mrow><mo>,</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow><mo>&amp;#x2212;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x00D7;</mo><mo stretchy=&quot;false&quot;>[</mo><mi>M</mi><mi>L</mi><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03C1;</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow><mo>&amp;#x2212;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;bold&quot;>F</mtext></mrow><mo stretchy=&quot;false&quot;>]</mo><mo>.</mo></math>" role="presentation" style="font-size: 113%; text-align: center; position: relative;"><span id="MJXc-Node-1302" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1303" class="mjx-mrow"><span id="MJXc-Node-1304" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-1305" class="mjx-texatom"><span id="MJXc-Node-1306" class="mjx-mrow"><span id="MJXc-Node-1307" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">F</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-1308" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span></span></span><span id="MJXc-Node-1309" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.058em; padding-bottom: 0.335em;">=</span></span><span id="MJXc-Node-1310" class="mjx-texatom MJXc-space3"><span id="MJXc-Node-1311" class="mjx-mrow"><span id="MJXc-Node-1312" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span><span id="MJXc-Node-1313" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.279em; padding-bottom: 0.445em;">−</span></span><span id="MJXc-Node-1314" class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.279em; padding-right: 0.045em;">C</span></span><span id="MJXc-Node-1315" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">o</span></span><span id="MJXc-Node-1316" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">n</span></span><span id="MJXc-Node-1317" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">v</span></span><span id="MJXc-Node-1318" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1319" class="mjx-texatom"><span id="MJXc-Node-1320" class="mjx-mrow"><span id="MJXc-Node-1321" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">K</span></span></span></span><span id="MJXc-Node-1322" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1323" class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span><span id="MJXc-Node-1324" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1325" class="mjx-texatom MJXc-space1"><span id="MJXc-Node-1326" class="mjx-mrow"><span id="MJXc-Node-1327" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">P</span></span></span></span><span id="MJXc-Node-1328" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1329" class="mjx-texatom MJXc-space1"><span id="MJXc-Node-1330" class="mjx-mrow"><span id="MJXc-Node-1331" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">F</span></span></span></span><span id="MJXc-Node-1332" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span><span id="MJXc-Node-1333" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.058em; padding-bottom: 0.335em;">=</span></span><span id="MJXc-Node-1334" class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.279em; padding-right: 0.045em;">C</span></span><span id="MJXc-Node-1335" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">o</span></span><span id="MJXc-Node-1336" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">n</span></span><span id="MJXc-Node-1337" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">v</span></span><span id="MJXc-Node-1338" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1339" class="mjx-texatom"><span id="MJXc-Node-1340" class="mjx-mrow"><span id="MJXc-Node-1341" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">K</span></span></span></span><span id="MJXc-Node-1342" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1343" class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.081em;">M</span></span><span id="MJXc-Node-1344" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">L</span></span><span id="MJXc-Node-1345" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.109em;">P</span></span><span id="MJXc-Node-1346" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1347" class="mjx-texatom"><span id="MJXc-Node-1348" class="mjx-mrow"><span id="MJXc-Node-1349" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span><span id="MJXc-Node-1350" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.279em; padding-bottom: 0.445em;">−</span></span><span id="MJXc-Node-1351" class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span><span id="MJXc-Node-1352" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span><span id="MJXc-Node-1353" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.224em; padding-bottom: 0.335em;">×</span></span><span id="MJXc-Node-1354" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">[</span></span><span id="MJXc-Node-1355" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.081em;">M</span></span><span id="MJXc-Node-1356" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">L</span></span><span id="MJXc-Node-1357" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.109em;"><span id="MJXc-Node-1358" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.109em;">P</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-1359" class="mjx-texatom" style=""><span id="MJXc-Node-1360" class="mjx-mrow"><span id="MJXc-Node-1361" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.501em;">ρ</span></span></span></span></span></span><span id="MJXc-Node-1362" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1363" class="mjx-texatom"><span id="MJXc-Node-1364" class="mjx-mrow"><span id="MJXc-Node-1365" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span><span id="MJXc-Node-1366" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.279em; padding-bottom: 0.445em;">−</span></span><span id="MJXc-Node-1367" class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.445em;">p</span></span><span id="MJXc-Node-1368" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span><span id="MJXc-Node-1369" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1370" class="mjx-texatom MJXc-space1"><span id="MJXc-Node-1371" class="mjx-mrow"><span id="MJXc-Node-1372" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.39em; padding-bottom: 0.39em;">F</span></span></span></span><span id="MJXc-Node-1373" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">]</span></span><span id="MJXc-Node-1374" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.335em;">.</span></span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">F</mtext></mrow><mi>p</mi></msub><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow><mo>−</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">K</mtext></mrow><mo>,</mo><mi>p</mi><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">P</mtext></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">F</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">K</mtext></mrow><mo>,</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow><mo>−</mo><mi>p</mi><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">[</mo><mi>M</mi><mi>L</mi><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>ρ</mi></mrow></msub><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow><mo>−</mo><mi>p</mi><mo stretchy="false">)</mo><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">F</mtext></mrow><mo stretchy="false">]</mo><mo>.</mo></math></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-76">\begin{equation}
\textbf{F}_p = \mathcal{X} - Conv(\textbf{K}, p, \textbf{P}, \textbf{F}) = Conv(\textbf{K}, MLP(\mathcal{X}-p) \times [MLP_{\rho}(\mathcal{X}-p), \textbf{F}].
\end{equation}</script></span></p><p><img src="https://i.imgur.com/KkMhGfV.png" alt=""><br>
<em><span>Figure 4: </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-77-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1375" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1376" class="mjx-mrow"><span id="MJXc-Node-1377" class="mjx-texatom"><span id="MJXc-Node-1378" class="mjx-mrow"><span id="MJXc-Node-1379" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-77">\mathcal{X}</script></span><span>-Conv operator</span><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></em></p><p><span>With the </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-78-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1380" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1381" class="mjx-mrow"><span id="MJXc-Node-1382" class="mjx-texatom"><span id="MJXc-Node-1383" class="mjx-mrow"><span id="MJXc-Node-1384" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-78">\mathcal{X}</script></span><span>-Conv operation as building blokes, PointCNN is able to perform convolutions on the unordered point set, similar to that of 2-dimension grids, as illustrated in </span><em><span>Figure 5</span></em><span>.</span></p><p><img src="https://i.imgur.com/yyNo6L4.png" alt=""><br>
<em><span>Figure 5: Hierarchical convolution on regular grids (upper) and point clouds (lower)</span><sup class="footnote-ref"><a href="#fn5" id="fnref5:1">[5:1]</a></sup></em></p><h3 id="Graph-Cut-and-GrabCut" data-id="Graph-Cut-and-GrabCut"><a class="anchor hidden-xs" href="#Graph-Cut-and-GrabCut" title="Graph-Cut-and-GrabCut"><span class="octicon octicon-link"></span></a><span>Graph Cut and GrabCut</span></h3><p><span>Graph cut, as an energy minimization technique, can be employed to solve many low-level computer vision problems. Let </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-79-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo>=&amp;lt;</mo><mi>N</mi><mo>,</mo><mi>L</mi><mo>&amp;gt;</mo></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1385" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1386" class="mjx-mrow"><span id="MJXc-Node-1387" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.279em;">G</span></span><span id="MJXc-Node-1388" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.279em; padding-bottom: 0.058em;">=<span class="mjx-charbox MJXc-TeX-main-R" style="padding-bottom: 0.314em;">&lt;</span></span></span><span id="MJXc-Node-1389" class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.085em;">N</span></span><span id="MJXc-Node-1390" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1391" class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">L</span></span><span id="MJXc-Node-1392" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.279em; padding-bottom: 0.39em;">&gt;</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi><mo>=&lt;</mo><mi>N</mi><mo>,</mo><mi>L</mi><mo>&gt;</mo></math></span></span><script type="math/tex" id="MathJax-Element-79">G = <N, L></script></span><span> be a graph formed by a set of nodes </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-80-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1393" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1394" class="mjx-mrow"><span id="MJXc-Node-1395" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.085em;">N</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-80">N</script></span><span> and a set of directed links </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-81-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1396" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1397" class="mjx-mrow"><span id="MJXc-Node-1398" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">L</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></span></span><script type="math/tex" id="MathJax-Element-81">L</script></span><span> that connect them. In </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-82-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1399" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1400" class="mjx-mrow"><span id="MJXc-Node-1401" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.085em;">N</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-82">N</script></span><span> there are two special </span><em><span>terminal</span></em><span> nodes which are defined as the </span><em><span>source</span></em><span> and the </span><em><span>sink</span></em><span>, and the other nodes are </span><em><span>non-terminal</span></em><span> nodes </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-83-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1402" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1403" class="mjx-mrow"><span id="MJXc-Node-1404" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.109em;">P</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi></math></span></span><script type="math/tex" id="MathJax-Element-83">P</script></span><span>. This kind of graph is called </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-84-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mo>&amp;#x2212;</mo><mi>t</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1405" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1406" class="mjx-mrow"><span id="MJXc-Node-1407" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">s</span></span><span id="MJXc-Node-1408" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.279em; padding-bottom: 0.445em;">−</span></span><span id="MJXc-Node-1409" class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.39em; padding-bottom: 0.279em;">t</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>−</mo><mi>t</mi></math></span></span><script type="math/tex" id="MathJax-Element-84">s-t</script></span><span> graph.</span></p><p><span>Image segmentation is generally regarded as a pixel labeling problem. For binary segmentation, two labels are to be assigned corresponding to pixels belonging to the background and the object/foreground. Inherently, the binary segmentation can be represented as a partition of the graph, which can be solved by graph cut operation where the two subsets </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-85-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>S</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1410" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1411" class="mjx-mrow"><span id="MJXc-Node-1412" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.279em; padding-right: 0.032em;">S</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi></math></span></span><script type="math/tex" id="MathJax-Element-85">S</script></span><span> and </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-86-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1413" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1414" class="mjx-mrow"><span id="MJXc-Node-1415" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.12em;">T</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-86">T</script></span><span> represent the foreground and background, as illustrated in </span><em><span>Figure 6</span></em><span>.</span></p><p><img src="https://i.imgur.com/kftr0jH.png" alt=""><br>
<em><span>Figure 6: Image segmentation as a graph cut problem</span></em></p><p><span>GrabCut is an image segmentation method based on graph cut, where minimal user interaction is needed compared to the original graph cut method</span><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup><span>. Starting with a user-specified bounding box around the object to be segmented, the algorithm uses a Gaussian mixture model to approximate the color distribution of the target object and that of the background. Taken from the bounding box, the segmentation can be further improved with user-assigned foreground/background pixels. This two-step procedure is repeated until convergence. </span><em><span>Figure 7</span></em><span> shows an example of the input and output of Grabcut.</span></p><p><img src="https://i.imgur.com/EiFXaBq.jpg" alt=""><br>
<em><span>Figure 7: An example of Grabcut. Taken from </span><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup><span>.</span></em></p><p><span>Though designed for interactive segmentation, the process of Grabcut can be simply automated by only providing the bounding box covering the whole image.</span></p><h2 id="Experimental-Setup" data-id="Experimental-Setup"><a class="anchor hidden-xs" href="#Experimental-Setup" title="Experimental-Setup"><span class="octicon octicon-link"></span></a><span>Experimental Setup</span></h2><h3 id="Configurations" data-id="Configurations"><a class="anchor hidden-xs" href="#Configurations" title="Configurations"><span class="octicon octicon-link"></span></a><span>Configurations</span></h3><h4 id="Hardware" data-id="Hardware"><a class="anchor hidden-xs" href="#Hardware" title="Hardware"><span class="octicon octicon-link"></span></a><span>Hardware</span></h4><ul>
<li><span>GPU: NVIDIA GTX 1660 with 6 GB VRAM</span></li>
<li><span>CPU: Intel i5-9400F</span></li>
<li><span>Memory: 16 GB</span></li>
</ul><h4 id="Requirements" data-id="Requirements"><a class="anchor hidden-xs" href="#Requirements" title="Requirements"><span class="octicon octicon-link"></span></a><span>Requirements</span></h4><ul>
<li><span>Ubuntu 18.04</span></li>
<li><span>PyTorch 1.2</span></li>
<li><span>PyTorch Geometric 1.5</span><br>
<span>Please refer to the </span><a href="https://github.com/chenzhaiyu/pointset-image-recognition/blob/master/environment.yml" target="_blank" rel="noopener"><span>environment.yml</span></a><span> for a complete list of required packages.</span></li>
</ul><h3 id="Datasets" data-id="Datasets"><a class="anchor hidden-xs" href="#Datasets" title="Datasets"><span class="octicon octicon-link"></span></a><span>Datasets</span></h3><ul>
<li>
<p><strong><span>MNIST:</span></strong><span> a database of handwritten digits from 0 to 9, containing a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28×28 gray-scale image. </span><em><span>Figure 8</span></em><span> shows some examples of the digits.</span><br><br>
<img src="https://i.imgur.com/N6er1JV.png" alt="" width="400"><br>
<em><span>Figure 8: Some examples in MNIST</span></em></p>
</li>
<li>
<p><strong><span class="ui-comment-inline-span">Fashion MNIST:</span></strong><span class="ui-comment-inline-span"> a direct extension</span><span> of MNIST, which shares the same image size and structure of training and testing splits with MNIST. The dataset includes another 10 classes associated with fashion – T-shirt/top, 	trouser, pullover, dress, coat, sandal, shirt, sneaker, bag and ankle boot, as shown in </span><em><span>Figure 9</span></em><span>.</span><br><br>
<img src="https://i.imgur.com/qYh8pgH.png" alt="" width="400"><br>
<em><span>Figure 9: A visualization of part of examples in Fashion MNIST, with each class taking three rows</span></em></p>
</li>
<li>
<p><strong><span>CIFAR10:</span></strong><span> a dataset consisting of 60,000 32x32 colour images in 10 classes, which are split into 50,000 training examples and 10,000 text examples. Specifically, the classes contained in the dataset are airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck, as shown in </span><em><span>Figure 10</span></em><span>. </span><br><br>
<img src="https://i.imgur.com/lfKmgWj.png" alt="" width="400"><br>
<em><span>Figure 10: Random examples of CIFAR10</span></em></p>
</li>
</ul><h3 id="Data-pre-processing-Convert-images-to-point-clouds" data-id="Data-pre-processing-Convert-images-to-point-clouds"><a class="anchor hidden-xs" href="#Data-pre-processing-Convert-images-to-point-clouds" title="Data-pre-processing-Convert-images-to-point-clouds"><span class="octicon octicon-link"></span></a><span>Data pre-processing: Convert images to point clouds</span></h3><ul>
<li>
<p><strong><span>MNIST/Fashion MNIST:</span></strong><span> Both datasets contain gray-scale images. Pixels having 0 value indicate the background. Thus, we can extract the foreground pixels by simply setting a threshold of 0. To further convert an image into a 2D point cloud, we follow the sampling method used by Qi et al. for PointNet</span><sup class="footnote-ref"><a href="#fn1" id="fnref1:4">[1:4]</a></sup><span>. A set size of 256 is used. If there are more than 256 foreground pixels in the set, we randomly sub-sample it; if there are less, we pad the set with the last pixel in the set. Then the output point cloud includes a series of (</span><em><span>x</span></em><span>, </span><em><span>y</span></em><span>, </span><em><span>z</span></em><span>) coordinates, with (</span><em><span>x</span></em><span>, </span><em><span>y</span></em><span>) assigned by (</span><em><span>row</span></em><span>, </span><em><span>col</span></em><span>) of each pixel and </span><em><span>z</span></em><span> = 0.0.</span></p>
</li>
<li>
<p><strong><span>CIFAR10:</span></strong><span> Images in CIFAR10 dataset have three channels with R, G, B features, so the shape information of the foreground object is actually “lost” . In this case, we just randomly sample 512 pixels from the whole image. Beside (</span><em><span>x</span></em><span>, </span><em><span>y</span></em><span>, </span><em><span>z</span></em><span>), we also add R, G, B values of each pixel as three additional features in the output point cloud. </span><em><span>Figure 11</span></em><span> shows the examples. Code for data conversion can also be found in the </span><a href="https://github.com/chenzhaiyu/pointset-image-recognition/tree/master/data_utils" target="_blank" rel="noopener"><span>data_util</span></a><span> directory. </span><br><br>
<img src="https://i.imgur.com/pcyLBJm.png" alt="" width="60"><span> -&gt;</span><img src="https://i.imgur.com/IuI1Kmm.png" alt="" width="100"><span>-&gt;</span><img src="https://i.imgur.com/vvBFste.png" alt="" width="100"><br>
<img src="https://i.imgur.com/Pz8EPE4.png" alt="" width="60"><span> -&gt;</span><img src="https://i.imgur.com/HKHDIaQ.png" alt="" width="100"><span>-&gt;</span><img src="https://i.imgur.com/fmgUFjV.png" alt="" width="100"><br>
<img src="https://i.imgur.com/uLThjTG.png" alt="" width="60"><span> -&gt;</span><img src="https://i.imgur.com/o87TvB8.png" alt="" width="100"><span>-&gt;</span><img src="https://i.imgur.com/YlPtY2A.png" alt="" width="100"><br>
<img src="https://i.imgur.com/sZ5ACBx.png" alt="" width="60"><span> -&gt;</span><img src="https://i.imgur.com/PhtQjYR.png" alt="" width="100"><span>-&gt;</span><img src="https://i.imgur.com/c23bx0S.png" alt="" width="100"><br>
<em><span>Figure 11: Examples of converting images to point clouds. Left: Original images in CIFAR10. Middle: Point clouds with all pixels preserved. Right: Point clouds with 512 pixels randomly sampled.</span></em></p>
</li>
</ul><h3 id="Hyperparameter-Settings" data-id="Hyperparameter-Settings"><a class="anchor hidden-xs" href="#Hyperparameter-Settings" title="Hyperparameter-Settings"><span class="octicon octicon-link"></span></a><span>Hyperparameter Settings</span></h3><ul>
<li><span>Optimizer: Adam</span></li>
<li><span>Learning Rate: 0.001</span></li>
<li><span>Learning Rate Scheduler</span>
<ul>
<li><span>Step Size: 20</span></li>
<li><span>Decay Rate: 0.7</span></li>
</ul>
</li>
<li><span>Number of Input Points</span>
<ul>
<li><span>MNIST: 256</span></li>
<li><span>FASHION MNIST: 256</span></li>
<li><span>CIFAR10 (without Grabcut): 512</span></li>
<li><span>CIFAR10 (with Grabcut): 256</span></li>
</ul>
</li>
</ul><h2 id="Results" data-id="Results"><a class="anchor hidden-xs" href="#Results" title="Results"><span class="octicon octicon-link"></span></a><span>Results</span></h2><h3 id="Image-Classification" data-id="Image-Classification"><a class="anchor hidden-xs" href="#Image-Classification" title="Image-Classification"><span class="octicon octicon-link"></span></a><span>Image Classification</span></h3><p><span>First we trained PointNet, PointNet++ and PointCNN for MNIST and Fashion MNIST classification, with results summarized in </span><em><span>Table 1</span></em><span>. In addition, </span><span class="ui-comment-inline-span">accuracies shown inside the brackets are acquired by the original papers.</span><span> It can be noticed that we managed to reproduce the results for MNIST using PointNet and PointNet++. In the case of PointCNN, the reason why we achieved much worse accuracy is that we are using a simple PyTorch implementation of PointCNN without complicated data augmentation used by the original paper.</span></p><p><span>From our results, PointNet++ shows a slight improvement over PointNet on MNIST dataset, which might be attributed to its hierarchical structure. For Fashion MNIST, however, the effect of such structure is not apparent anymore.</span></p><p><span>We also compare the classification results with some existed baseline using 2D CNNs </span><span class="ui-comment-inline-span">(summarized </span><a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener"><span class="ui-comment-inline-span">here</span></a><span class="ui-comment-inline-span"> and </span><a href="https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" target="_blank" rel="noopener"><span class="ui-comment-inline-span">here</span></a><span class="ui-comment-inline-span">)</span><span>, as shown with (*). When classifying MNIST dataset, networks for point clouds have shown comparable results with 2D CNNs. However, this is not the case for Fashion MNIST. When the image becomes more complex, 2D CNNs win undoubtedly.</span></p><p><em><span class="ui-comment-inline-span">Table 1. Test accuracy (%) for image classification on MNIST and Fashion MNIST</span></em></p><table>
<thead>
<tr>
<th style="text-align:left"><span>Network</span></th>
<th style="text-align:center"><span>MNIST</span></th>
<th style="text-align:center"><span>Fashion MNIST</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><span>PointNet</span></td>
<td style="text-align:center"><span>99.21 (99.22)</span></td>
<td style="text-align:center"><span>84.75</span></td>
</tr>
<tr>
<td style="text-align:left"><span>PointNet++</span></td>
<td style="text-align:center"><span>99.52 (99.49)</span></td>
<td style="text-align:center"><span>84.64</span></td>
</tr>
<tr>
<td style="text-align:left"><span>PointCNN</span></td>
<td style="text-align:center"><span>98.75 (99.54)</span></td>
<td style="text-align:center"><span>77.53</span></td>
</tr>
<tr>
<td style="text-align:left"><span>2 Conv+3 FC ~ 500K parameters (*)</span></td>
<td style="text-align:center"><span>99.40</span></td>
<td style="text-align:center"><span>93.40</span></td>
</tr>
<tr>
<td style="text-align:left"><span>3 Conv+pooling+BN (*)</span></td>
<td style="text-align:center"><span>99.40</span></td>
<td style="text-align:center"><span>90.30</span></td>
</tr>
<tr>
<td style="text-align:left"><span>ResNet18</span><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup><span>*</span></td>
<td style="text-align:center"><span>97.90</span></td>
<td style="text-align:center"><span>94.90</span></td>
</tr>
<tr>
<td style="text-align:left"><span>Maxout</span><sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup><span>*</span></td>
<td style="text-align:center"><span>99.55</span></td>
<td style="text-align:center"><span>-</span></td>
</tr>
<tr>
<td style="text-align:left"><span>Network in Network</span><sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup><span>*</span></td>
<td style="text-align:center"><span>99.53</span></td>
<td style="text-align:center"><span>-</span></td>
</tr>
</tbody>
</table><p><em><span>Table 2</span></em><span> summarizes the results for classifying a set of images containing more various objects and additional RGB features – CIFAR10. In this case, 2D CNNs (*) can achieve much better classification results than networks for point clouds.</span></p><p><em><span class="ui-comment-inline-span"><span class="ui-comment-inline-span">Table 2. Test accuracy (%) for image classification on CIFAR10</span></span></em></p><table>
<thead>
<tr>
<th style="text-align:left"><span>Network</span></th>
<th style="text-align:center"><span>CIFAR10</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><span>PointNet</span></td>
<td style="text-align:center"><span>11.02</span></td>
</tr>
<tr>
<td style="text-align:left"><span>PointNet++</span></td>
<td style="text-align:center"><span>10.19 (10.00)</span></td>
</tr>
<tr>
<td style="text-align:left"><span>PointCNN</span></td>
<td style="text-align:center"><span>71.06 (80.22)</span></td>
</tr>
<tr>
<td style="text-align:left"><span>Maxout</span><sup class="footnote-ref"><a href="#fn9" id="fnref9:1">[9:1]</a></sup><span>*</span></td>
<td style="text-align:center"><span>90.65</span></td>
</tr>
<tr>
<td style="text-align:left"><span>Network in Network</span><sup class="footnote-ref"><a href="#fn10" id="fnref10:1">[10:1]</a></sup><span>*</span></td>
<td style="text-align:center"><span>91.20</span></td>
</tr>
<tr>
<td style="text-align:left"><span>ResNet110</span><sup class="footnote-ref"><a href="#fn8" id="fnref8:1">[8:1]</a></sup><span>*</span></td>
<td style="text-align:center"><span>93.57</span></td>
</tr>
</tbody>
</table><p><span>Among PointNet, PointNet++ and PointCNN, it’s not surprising that PointNet and PointNet++ completely failed on this task, with the accuracy (around 10%) no better than random choice. Considering that CIFAR10 actually misses the shape information of foreground objects – the background and foreground in one image are just indistinguishable without a clear threshold pixel value (e.g., 0 in MNIST and Fashion MNIST), it would be really hard for networks like PointNet and PointNet++ which do not use local information to extract foreground information. As for PointCNN, the convolution technique used in this architecture helps a lot to capture local information in image points, resulting in way higher accuracy on CIFAR10.</span></p><h3 id="RGB-vs-Shape" data-id="RGB-vs-Shape"><a class="anchor hidden-xs" href="#RGB-vs-Shape" title="RGB-vs-Shape"><span class="octicon octicon-link"></span></a><span>RGB vs. Shape</span></h3><p><span>As mentioned in the results for CIFAR10 classification, the loss of shape information may “hinder” the learning of PointNet and PointNet++. This leads us to ask a question: would it be helpful if we manually extract the foreground of CIFAR10 images? Looking at the results in </span><em><span>Table 3</span></em><span>, the answer would be “yes”.</span></p><p><span>After applying Grabcut to the original image dataset, both PointNet and PointNet++ have a large improvement over the results without shape information. PointNet++ also shows an advantage over PointNet here. Using these comparisons in Table 3, we can also draw the following conclusion:</span></p><ul>
<li><span>PointNet and PointNet++ rely more on shape information instead of RGB features.</span></li>
</ul><p><em><span>Table 3. Test accuracy (%) for CIFAR10 classification without/with Grabcut</span></em></p><table>
<thead>
<tr>
<th style="text-align:left"><span>Network</span></th>
<th style="text-align:center"><span>CIFAR10</span></th>
<th style="text-align:center"><span>CIFAR10+Grabcut</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><span>PointNet</span></td>
<td style="text-align:center"><span>11.02</span></td>
<td style="text-align:center"><span>32.37</span></td>
</tr>
<tr>
<td style="text-align:left"><span>PointNet++</span></td>
<td style="text-align:center"><span>10.19</span></td>
<td style="text-align:center"><span>34.89</span></td>
</tr>
</tbody>
</table><p><span>Another note should also be made for Grabcut. Since we are using the built-in function in OpenCV here, some of the extractions can be false or even blank if there is not apparent contrast between the background and the foreground object (</span><em><span>Figure 12</span></em><span>).</span></p><p><img src="https://i.imgur.com/beqfAzR.png" alt="" width="100"><img src="https://i.imgur.com/IiarcWZ.png" alt="" width="100"><span> &nbsp; </span><img src="https://i.imgur.com/bPvydWw.png" alt="" width="100"><img src="https://i.imgur.com/8wa3E5Q.png" alt="" width="100"><br>
<span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(a) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (b)</span><br>
<em><span>Figure 12: (a) A successful example and (b) a “failed” example of Grabcut</span></em></p><p><span>Although we tried to mitigate this problem by only keeping extracted images with more than 10% pixels compared to the original image, the classification results can still be affected by such issues of Grabcut algorithm.</span></p><h3 id="Is-PointCNN-Shift-Invariance" data-id="Is-PointCNN-Shift-Invariance"><a class="anchor hidden-xs" href="#Is-PointCNN-Shift-Invariance" title="Is-PointCNN-Shift-Invariance"><span class="octicon octicon-link"></span></a><span>Is PointCNN Shift Invariance?</span></h3><p><img src="https://i.imgur.com/kRLRzKF.png" alt="" width="73"><img src="https://i.imgur.com/q1wvGs4.png" alt="" width="73"><img src="https://i.imgur.com/TKQ43UD.png" alt="" width="73"><img src="https://i.imgur.com/SIqA0rz.png" alt="" width="73"><img src="https://i.imgur.com/6DSLyEG.png" alt="" width="73"><img src="https://i.imgur.com/dlyo5ei.png" alt="" width="73"><img src="https://i.imgur.com/LfAxpdR.png" alt="" width="73"><img src="https://i.imgur.com/E5LrUS9.png" alt="" width="73"><br>
<em><span>Figure 13: Shift a biplane with the number of pixels from 1 to 8 (from left to right)</span></em></p><p><span>To investigate if PointCNN shows shift invariance on images, we shifted the center of CIFAR10 images by 1 to 8 pixels (</span><em><span>Figure 13</span></em><span>) and cropped the images from 32×32 to 25×25. Then these “shifted” images are fed into PointCNN (trained on original CIFAR10 dataset) for testing. </span><em><span>Figure 14</span></em><span> shows how the prediction probability changes with the shifts.</span></p><p><img src="https://i.imgur.com/M06LLLs.png" alt=""></p><p><em><span>Figure 14: Changes of predicted probability of the correct class using PointCNN when shifting the image (with a comparison with VGG16</span><sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup><span>).</span></em></p><p><span>We check how often the network outputs the same classification, given the same image with two different </span><span class="ui-comment-inline-span">shifts</span><span>: </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-90-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>E</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mo>,</mo><mi>h</mi><mn>1</mn><mo>,</mo><mi>w</mi><mn>1</mn><mo>,</mo><mi>h</mi><mn>2</mn><mo>,</mo><mi>w</mi><mn>2</mn></mrow></msub><mn>1</mn><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>h</mi><mn>1</mn><mo>,</mo><mi>w</mi><mn>1</mn></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>h</mi><mn>2</mn><mo>,</mo><mi>w</mi><mn>2</mn></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1499" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1500" class="mjx-mrow"><span id="MJXc-Node-1501" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span id="MJXc-Node-1502" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.026em;">E</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span id="MJXc-Node-1503" class="mjx-texatom" style=""><span id="MJXc-Node-1504" class="mjx-mrow"><span id="MJXc-Node-1505" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.024em;">X</span></span><span id="MJXc-Node-1506" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1507" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">h</span></span><span id="MJXc-Node-1508" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">1</span></span><span id="MJXc-Node-1509" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1510" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">w</span></span><span id="MJXc-Node-1511" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">1</span></span><span id="MJXc-Node-1512" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1513" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">h</span></span><span id="MJXc-Node-1514" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">2</span></span><span id="MJXc-Node-1515" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1516" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">w</span></span><span id="MJXc-Node-1517" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">2</span></span></span></span></span></span><span id="MJXc-Node-1518" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">1</span></span><span id="MJXc-Node-1519" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">{</span></span><span id="MJXc-Node-1520" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">a</span></span><span id="MJXc-Node-1521" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">r</span></span><span id="MJXc-Node-1522" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.501em; padding-right: 0.003em;">g</span></span><span id="MJXc-Node-1523" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">m</span></span><span id="MJXc-Node-1524" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">a</span></span><span id="MJXc-Node-1525" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">x</span></span><span id="MJXc-Node-1526" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.109em;">P</span></span><span id="MJXc-Node-1527" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1528" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.279em; padding-right: 0.032em;">S</span></span><span id="MJXc-Node-1529" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">h</span></span><span id="MJXc-Node-1530" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">i</span></span><span id="MJXc-Node-1531" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.501em; padding-right: 0.06em;">f</span></span><span id="MJXc-Node-1532" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-1533" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.39em; padding-bottom: 0.279em;">t</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span id="MJXc-Node-1534" class="mjx-texatom" style=""><span id="MJXc-Node-1535" class="mjx-mrow"><span id="MJXc-Node-1536" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">h</span></span><span id="MJXc-Node-1537" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">1</span></span><span id="MJXc-Node-1538" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1539" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">w</span></span><span id="MJXc-Node-1540" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">1</span></span></span></span></span></span><span id="MJXc-Node-1541" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1542" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.024em;">X</span></span><span id="MJXc-Node-1543" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span><span id="MJXc-Node-1544" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span><span id="MJXc-Node-1545" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.058em; padding-bottom: 0.335em;">=</span></span><span id="MJXc-Node-1546" class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">a</span></span><span id="MJXc-Node-1547" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">r</span></span><span id="MJXc-Node-1548" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.501em; padding-right: 0.003em;">g</span></span><span id="MJXc-Node-1549" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">m</span></span><span id="MJXc-Node-1550" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">a</span></span><span id="MJXc-Node-1551" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">x</span></span><span id="MJXc-Node-1552" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.109em;">P</span></span><span id="MJXc-Node-1553" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1554" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.279em; padding-right: 0.032em;">S</span></span><span id="MJXc-Node-1555" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">h</span></span><span id="MJXc-Node-1556" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">i</span></span><span id="MJXc-Node-1557" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.501em; padding-bottom: 0.501em; padding-right: 0.06em;">f</span></span><span id="MJXc-Node-1558" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-1559" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.39em; padding-bottom: 0.279em;">t</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span id="MJXc-Node-1560" class="mjx-texatom" style=""><span id="MJXc-Node-1561" class="mjx-mrow"><span id="MJXc-Node-1562" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em;">h</span></span><span id="MJXc-Node-1563" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">2</span></span><span id="MJXc-Node-1564" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.163em; padding-bottom: 0.556em;">,</span></span><span id="MJXc-Node-1565" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.224em; padding-bottom: 0.279em;">w</span></span><span id="MJXc-Node-1566" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em; padding-bottom: 0.335em;">2</span></span></span></span></span></span><span id="MJXc-Node-1567" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">(</span></span><span id="MJXc-Node-1568" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.445em; padding-bottom: 0.279em; padding-right: 0.024em;">X</span></span><span id="MJXc-Node-1569" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span><span id="MJXc-Node-1570" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">)</span></span><span id="MJXc-Node-1571" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.445em; padding-bottom: 0.611em;">}</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>X</mi><mo>,</mo><mi>h</mi><mn>1</mn><mo>,</mo><mi>w</mi><mn>1</mn><mo>,</mo><mi>h</mi><mn>2</mn><mo>,</mo><mi>w</mi><mn>2</mn></mrow></msub><mn>1</mn><mo fence="false" stretchy="false">{</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><msub><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>h</mi><mn>1</mn><mo>,</mo><mi>w</mi><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><msub><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>h</mi><mn>2</mn><mo>,</mo><mi>w</mi><mn>2</mn></mrow></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo fence="false" stretchy="false">}</mo></math></span></span><script type="math/tex" id="MathJax-Element-90">E_{X, h1, w1, h2, w2}1\{argmaxP(Shift_{h1, w1}(X)) = argmaxP(Shift_{h2, w2}(X))\}</script></span><span>, as introduced in </span><sup class="footnote-ref"><a href="#fn11" id="fnref11:1">[11:1]</a></sup><span> as the consistency score. Table x shows the result. Compared with VGG16, PointNet exhibits significantly lower consistency.</span></p><h2 id="Extension-Image-Segmentation" data-id="Extension-Image-Segmentation"><a class="anchor hidden-xs" href="#Extension-Image-Segmentation" title="Extension-Image-Segmentation"><span class="octicon octicon-link"></span></a><span>Extension: Image Segmentation</span></h2><p><span>We also extend our experiments to image segmentation using PointNet and PointNet++, which has not been discussed in previous works. To train 2D point clouds for semantic segmentation, we choose PASCAL VOC 2012, which is an image dataset designed for several recognition tasks (e.g. image classification, semantic segmentation, action classification, etc.) and contains 20 classes.</span></p><p><img src="https://i.imgur.com/ltiws5Q.jpg" alt="" width="300"><br>
<em><span>Figure 15: Two examples for semantic segmentation in PASCAL VOC 2012</span></em></p><p><span>Converting PASCAL VOC 2012 images to point clouds is similar to the method we used for CIFAR10. Considering the image size, 4096 pixels are randomly sampled from each image. </span><em><span>Table 4</span></em><span> shows the mean </span><em><span>IOU</span></em><span> of image segmentation using PointNet and PointNet++. It can be noticed that we have not achieved good results in this task, especially when compared to 2D CNNs listed in </span><em><span>Table 4</span></em><span>. During random rampling, a lot of local information contained in the image can be left out. Such sampling method actually causes insufficient point density at objects with semantic meaning (e.g. the person on the motor bike in </span><em><span>Figure 15</span></em><span>), which can largely affect the segmentation result.</span></p><p><em><span>Table 4. Test mIOU (%) for semantic segmentation on PASCAL VOC 2012</span></em></p><table>
<thead>
<tr>
<th style="text-align:left"><span>Network</span></th>
<th style="text-align:center"><span>PASCAL VOC 2012</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><span>PointNet</span></td>
<td style="text-align:center"><span>9.36</span></td>
</tr>
<tr>
<td style="text-align:left"><span>PointNet++</span></td>
<td style="text-align:center"><span>14.54</span></td>
</tr>
<tr>
<td style="text-align:left"><span>FCN-8s</span><sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup><span>*</span></td>
<td style="text-align:center"><span>62.2</span></td>
</tr>
<tr>
<td style="text-align:left"><span>SANet</span><sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup><span>*</span></td>
<td style="text-align:center"><span>83.2</span></td>
</tr>
<tr>
<td style="text-align:left"><span>DeepLabv3+</span><sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup><span>*</span></td>
<td style="text-align:center"><span>89.0</span></td>
</tr>
</tbody>
</table><h2 id="Discussion" data-id="Discussion"><a class="anchor hidden-xs" href="#Discussion" title="Discussion"><span class="octicon octicon-link"></span></a><span>Discussion</span></h2><h3 id="Data-Augmentation-in-PointCNN" data-id="Data-Augmentation-in-PointCNN"><a class="anchor hidden-xs" href="#Data-Augmentation-in-PointCNN" title="Data-Augmentation-in-PointCNN"><span class="octicon octicon-link"></span></a><span>Data Augmentation in PointCNN</span></h3><blockquote>
<p><em><span>To train the parameters in </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-88-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1489" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1490" class="mjx-mrow"><span id="MJXc-Node-1491" class="mjx-texatom"><span id="MJXc-Node-1492" class="mjx-mrow"><span id="MJXc-Node-1493" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-88">\mathcal{X}</script></span><span>-Conv, it is evidently not beneficial to keep using the same set of neighboring points, in the same order, for a specific representative point. To improve generalization, we propose to randomly sample and shuffle the input points, such that both the neighboring point sets and order may differ from batch to batch</span><sup class="footnote-ref"><a href="#fn5" id="fnref5:2">[5:2]</a></sup><span>.</span></em></p>
</blockquote><p><span>Due to the fact that the point cloud training data are produced by one-shot sampling from the images, instead of generated on the fly with each batch, the neighboring point sets are fixed from batch to batch, thus the generalization effect is limited and eventually the performance is degraded from the reported. Notice that, however, the input points are shuffled before being fed into the network, while through our ablation experiments it adds no significant performance.</span></p><h3 id="Evaluation-of-Shift-Invariance" data-id="Evaluation-of-Shift-Invariance"><a class="anchor hidden-xs" href="#Evaluation-of-Shift-Invariance" title="Evaluation-of-Shift-Invariance"><span class="octicon octicon-link"></span></a><span>Evaluation of Shift Invariance</span></h3><p><span>The consistency score proposed by </span><sup class="footnote-ref"><a href="#fn11" id="fnref11:2">[11:2]</a></sup><span> is strongly affected by the accuracy, i.e., when accuracy is significantly high (e.g. 99%), the consistency, which measures how often the network outputs the same classification given the same image with two different shifts, will be inevitably high as well. This dependent metric cannot depict the invariance under varied accuracy. Therefore considering the significantly gaped accuracy between PointCNN and VGG16, the conclusion is difficult to make: which one is more shift invariant? By this we also </span><span class="ui-comment-inline-span">question</span><span> the possibly inappropriate use of unorthogonal metrics in the paper</span><sup class="footnote-ref"><a href="#fn11" id="fnref11:3">[11:3]</a></sup><span> on “Making the Convolutional Neural Networks Shift Invariant Again”.</span></p><h3 id="Limitations-of-Our-Method" data-id="Limitations-of-Our-Method"><a class="anchor hidden-xs" href="#Limitations-of-Our-Method" title="Limitations-of-Our-Method"><span class="octicon octicon-link"></span></a><span>Limitations of Our Method</span></h3><p><span>Although extensive experiments and result analysis are provided, our method for image recognition using networks for point clouds still show some limitations:</span></p><ul>
<li><span>Datasets we mainly used in the project (i.e., MNIST, Fashion MNIST and CIFAR10) contain images with a small size and a very limited number of categories.</span></li>
<li><span>The random sampling method does not capture the shape information of objects in images in the best way and can cause an insufficient point density for the foreground.</span></li>
<li><span>We used a very basic implementation of PointCNN, which affects the performance and further comparisons.</span></li>
<li><span>We did not realize image segmentation on PointCNN.</span></li>
</ul><h2 id="Conclusion" data-id="Conclusion"><a class="anchor hidden-xs" href="#Conclusion" title="Conclusion"><span class="octicon octicon-link"></span></a><span>Conclusion</span></h2><p><span>In this project we explored the possibility of using neural networks designed for point cloud analysis, namely PointNet, PointNet++ and PointCNN in image recognition, with an emphasis on image classification.</span></p><p><span>With extensive experiments, the results show that for images where shape information is prominent (e.g., MNIST and Fashion MNIST), all of the three networks are capable of classifying them. For images without explicit shape information (e.g., CIFAR10), only PointCNN can learn solid feature representation, while PointNet and PointNet++ fail completely. By extracting the objects from the backgrounds to facilitate shape information expression, both PointNet and PointNet++ shows significant accuracy improvement, which indicates shape information is superior than the RGB for PointNet and PointNet++.</span></p><p><span>From the comparison with the baselines, it is clear that the CNNs have a considerable advantage over all the three networks designed for point cloud analysis. Notice that PointCNN employs the CNN mechanism by performing the </span><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span id="MathJax-Element-89-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>X</mi></mrow></math>" role="presentation" style="font-size: 113%; position: relative;"><span id="MJXc-Node-1494" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-1495" class="mjx-mrow"><span id="MJXc-Node-1496" class="mjx-texatom"><span id="MJXc-Node-1497" class="mjx-mrow"><span id="MJXc-Node-1498" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.445em; padding-bottom: 0.335em; padding-right: 0.094em;">X</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">X</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-89">\mathcal{X}</script></span><span>-conv, to which we attribute the “success” of PointCNN on CIFAR10 dataset. For the extended segmentation task, the gap between 2D CNNs and PointNet(++) is even larger. Therefore we conclude 2D CNNs are still dominant in image recognition.</span></p><p><span>For PointCNN, its consistency on shift invariance is investigated. We expected the consistency gain due to the random sampling which poses regularization to the network, while the result shows lower consistency compared to a 2D CNN, VGG16. Arguably the consistency metric is constraint by the accuracy. We also regard the low consistency as a consequence of the one-shot sampling mechanism which prevents the network to sufficiently learn the randomly distributed features from batch to batch.</span></p><h2 id="References" data-id="References"><a class="anchor hidden-xs" href="#References" title="References"><span class="octicon octicon-link"></span></a><span class="ui-comment-inline-span">References</span></h2><hr class="footnotes-sep"><section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><span>Qi, C. R., Su, H., Mo, K., &amp; Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 652-660).</span> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a> <a href="#fnref1:2" class="footnote-backref">↩︎</a> <a href="#fnref1:3" class="footnote-backref">↩︎</a> <a href="#fnref1:4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><span>Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</span> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><span>Ferguson, M., Ak, R., Lee, Y. T. T., &amp; Law, K. H. (2017, December). Automatic localization of casting defects with convolutional neural networks. In 2017 IEEE international conference on big data (big data) (pp. 1726-1735). IEEE.</span> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><span>Qi, C. R., Yi, L., Su, H., &amp; Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural information processing systems (pp. 5099-5108).</span> <a href="#fnref4" class="footnote-backref">↩︎</a> <a href="#fnref4:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><span>Li, Y., Bu, R., Sun, M., Wu, W., Di, X., &amp; Chen, B. (2018). Pointcnn: Convolution on x-transformed points. In Advances in neural information processing systems (pp. 820-830).</span> <a href="#fnref5" class="footnote-backref">↩︎</a> <a href="#fnref5:1" class="footnote-backref">↩︎</a> <a href="#fnref5:2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><span>Rother, C., Kolmogorov, V., &amp; Blake, A. (2004). “GrabCut” interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), 309-314.</span> <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><span>OpenCV Documentation. Interactive Foreground Extraction using GrabCut Algorithm, URL: </span><a href="https://docs.opencv.org/trunk/d8/d83/tutorial_py_grabcut.html" target="_blank" rel="noopener"><span>https://docs.opencv.org/trunk/d8/d83/tutorial_py_grabcut.html</span></a><span>. Accessed at June 2020.</span> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p><span>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</span> <a href="#fnref8" class="footnote-backref">↩︎</a> <a href="#fnref8:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><span>Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., &amp; Bengio, Y. (2013, February). Maxout networks. In International conference on machine learning (pp. 1319-1327).</span> <a href="#fnref9" class="footnote-backref">↩︎</a> <a href="#fnref9:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p><span>Lin, M., Chen, Q., &amp; Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.</span> <a href="#fnref10" class="footnote-backref">↩︎</a> <a href="#fnref10:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><span>Zhang, R. (2019). Making convolutional networks shift-invariant again. arXiv preprint arXiv:1904.11486.</span> <a href="#fnref11" class="footnote-backref">↩︎</a> <a href="#fnref11:1" class="footnote-backref">↩︎</a> <a href="#fnref11:2" class="footnote-backref">↩︎</a> <a href="#fnref11:3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p><span>Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.</span> <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><span>Zhong Z, Lin Z Q, Bidart R, et al. Squeeze-and-Attention Networks for Semantic Segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 13065-13074.</span> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p><span>Chen L C, Zhu Y, Papandreou G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 801-818.</span> <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Go-One-Dimension-Higher-Can-Neural-Networks-for-Point-Cloud-Analysis-Help-in-Image-Recognition" title="Go One Dimension Higher: Can Neural Networks for Point Cloud Analysis Help in Image Recognition?">Go One Dimension Higher: Can Neural Networks for Point Cloud Analysis Help in Image Recognition?</a><ul class="nav">
<li class=""><a href="#Related-Work" title="Related Work">Related Work</a><ul class="nav">
<li class=""><a href="#Image-Recognition" title="Image Recognition">Image Recognition</a></li>
<li class=""><a href="#Point-Cloud-Analysis" title="Point Cloud Analysis">Point Cloud Analysis</a></li>
<li class=""><a href="#Graph-Cut-and-GrabCut" title="Graph Cut and GrabCut">Graph Cut and GrabCut</a></li>
</ul>
</li>
<li class=""><a href="#Experimental-Setup" title="Experimental Setup">Experimental Setup</a><ul class="nav">
<li class=""><a href="#Configurations" title="Configurations">Configurations</a></li>
<li class=""><a href="#Datasets" title="Datasets">Datasets</a></li>
<li class=""><a href="#Data-pre-processing-Convert-images-to-point-clouds" title="Data pre-processing: Convert images to point clouds">Data pre-processing: Convert images to point clouds</a></li>
<li class=""><a href="#Hyperparameter-Settings" title="Hyperparameter Settings">Hyperparameter Settings</a></li>
</ul>
</li>
<li class=""><a href="#Results" title="Results">Results</a><ul class="nav">
<li class=""><a href="#Image-Classification" title="Image Classification">Image Classification</a></li>
<li class=""><a href="#RGB-vs-Shape" title="RGB vs. Shape">RGB vs. Shape</a></li>
<li class=""><a href="#Is-PointCNN-Shift-Invariance" title="Is PointCNN Shift Invariance?">Is PointCNN Shift Invariance?</a></li>
</ul>
</li>
<li class=""><a href="#Extension-Image-Segmentation" title="Extension: Image Segmentation">Extension: Image Segmentation</a></li>
<li class=""><a href="#Discussion" title="Discussion">Discussion</a><ul class="nav">
<li class=""><a href="#Data-Augmentation-in-PointCNN" title="Data Augmentation in PointCNN">Data Augmentation in PointCNN</a></li>
<li class=""><a href="#Evaluation-of-Shift-Invariance" title="Evaluation of Shift Invariance">Evaluation of Shift Invariance</a></li>
<li class=""><a href="#Limitations-of-Our-Method" title="Limitations of Our Method">Limitations of Our Method</a></li>
</ul>
</li>
<li class=""><a href="#Conclusion" title="Conclusion">Conclusion</a></li>
<li class=""><a href="#References" title="References">References</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;" null null>
        <div class="toc"><ul class="nav">
<li class=""><a href="#Go-One-Dimension-Higher-Can-Neural-Networks-for-Point-Cloud-Analysis-Help-in-Image-Recognition" title="Go One Dimension Higher: Can Neural Networks for Point Cloud Analysis Help in Image Recognition?">Go One Dimension Higher: Can Neural Networks for Point Cloud Analysis Help in Image Recognition?</a><ul class="nav">
<li class=""><a href="#Related-Work" title="Related Work">Related Work</a><ul class="nav">
<li class=""><a href="#Image-Recognition" title="Image Recognition">Image Recognition</a></li>
<li class=""><a href="#Point-Cloud-Analysis" title="Point Cloud Analysis">Point Cloud Analysis</a></li>
<li class=""><a href="#Graph-Cut-and-GrabCut" title="Graph Cut and GrabCut">Graph Cut and GrabCut</a></li>
</ul>
</li>
<li class=""><a href="#Experimental-Setup" title="Experimental Setup">Experimental Setup</a><ul class="nav">
<li class=""><a href="#Configurations" title="Configurations">Configurations</a></li>
<li class=""><a href="#Datasets" title="Datasets">Datasets</a></li>
<li class=""><a href="#Data-pre-processing-Convert-images-to-point-clouds" title="Data pre-processing: Convert images to point clouds">Data pre-processing: Convert images to point clouds</a></li>
<li class=""><a href="#Hyperparameter-Settings" title="Hyperparameter Settings">Hyperparameter Settings</a></li>
</ul>
</li>
<li class=""><a href="#Results" title="Results">Results</a><ul class="nav">
<li class=""><a href="#Image-Classification" title="Image Classification">Image Classification</a></li>
<li class=""><a href="#RGB-vs-Shape" title="RGB vs. Shape">RGB vs. Shape</a></li>
<li class=""><a href="#Is-PointCNN-Shift-Invariance" title="Is PointCNN Shift Invariance?">Is PointCNN Shift Invariance?</a></li>
</ul>
</li>
<li class=""><a href="#Extension-Image-Segmentation" title="Extension: Image Segmentation">Extension: Image Segmentation</a></li>
<li class=""><a href="#Discussion" title="Discussion">Discussion</a><ul class="nav">
<li class=""><a href="#Data-Augmentation-in-PointCNN" title="Data Augmentation in PointCNN">Data Augmentation in PointCNN</a></li>
<li class=""><a href="#Evaluation-of-Shift-Invariance" title="Evaluation of Shift Invariance">Evaluation of Shift Invariance</a></li>
<li class=""><a href="#Limitations-of-Our-Method" title="Limitations of Our Method">Limitations of Our Method</a></li>
</ul>
</li>
<li class=""><a href="#Conclusion" title="Conclusion">Conclusion</a></li>
<li class=""><a href="#References" title="References">References</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
